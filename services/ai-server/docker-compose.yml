services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    ports:
      - 11434:11434 # Web UI
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/ollama/data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                - gpu
    networks:
      - ai-network
  ui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openweb-ui
    restart: always
    ports:
      - 7000:8080 # Web UI
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/ollama/open-webui:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      # - "ENABLE_SIGNUP=false"
      - "OLLAMA_BASE_URL=http://ollama:11434"
      - DATABASE_URL=${OPENWEBUI_DB_STRING}
      # - QDRANT_API_KEY=${QDRANT_API_TOKEN}
      # - QDRANT_URI=http://qdrant:6333
      # - VECTOR_DB=chroma
      # - CHROMA_HTTP_HOST=http://chromadb
      # # - CHROMA_DATABASE=openwebuid-db
      # - CHROMA_HTTP_PORT=5001
      # - CHROMA_HTTP_HEADERS="Authorization=Bearer ${CHROMEDB_API_TOKEN},User-Agent=OpenWebUI"
      # - CHROMA_HTTP_SSL=False
      # - CHROMA_CLIENT_AUTH_PROVIDER=chromadb.auth.token_authn.TokenAuthClientProvider
      # - CHROMA_CLIENT_AUTH_CREDENTIALS=${CHROMEDB_API_TOKEN}
      - RAG_EMBEDDING_ENGINE=ollama
      - RAG_EMBEDDING_MODEL=nomic-embed-text-v1.5
      - RAG_EMBEDDING_MODEL_TRUST_REMOTE_CODE=true
      - RAG_TOP_K=5
      - ENABLE_WEBSOCKET_SUPPORT=true
      - WEBSOCKET_MANAGER=redis
      - WEBSOCKET_REDIS_URL=redis://redis:6379/1
      - CONTENT_EXTRACTION_ENGINE=tika
      - TIKA_SERVER_URL=http://host.docker.internal:9998
    networks:
      - ai-network
    depends_on:
      - ollama
      - chromadb
      - redis
      - vector-admin-postgres
  mcpo-tools:
    image: ghcr.io/open-webui/mcpo:main
    container_name: mcpo-tools
    restart: unless-stopped
    ports:
      - "8081:8001" # Exposing mcpo on host port 7010, container runs on 8001
    volumes:
      - ./mcp-servers/mcp_config.json:/mcp/mcp_config.json:ro # Mount the config file read-only
    command: > # Using > for multi-line command readability
      --host 0.0.0.0
      --port 8001
      --config /mcp/mcp_config.json
      --api-key ${OPENWEBUI_MCPO_API_TOKEN}
    depends_on:
      - ui
    networks:
      - ai-network
  litellm: # Template: https://docs.litellm.ai/docs/proxy/deploy#docker-compose
    build:
      context: .
      args:
        target: runtime
    env_file:
      - .env # Load local .env file
    image: ghcr.io/berriai/litellm:main-stable
    environment:
      DATABASE_URL: "${LITELLM_DB_CONNECTION}"
      STORE_MODEL_IN_DB: "True" # allows adding models to proxy via UI
    ports:
      - "7001:4000" # Map the container port to the host, change the host port if necessary
    # volumes:
      # - ./litellm-config.yaml:/app/config.yaml # Mount the local configuration file
    # You can change the port or number of workers as per your requirements or pass any new supported CLI augument. Make sure the port passed here matches with the container port defined above in `ports` value
    # "--config", "/app/config.yaml", 
    command: [ "--port", "4000", "--num_workers", "3" ]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - vector-admin-postgres
    networks:
      - ai-network
    healthcheck:  # Defines the health check configuration for the container
      test: [ "CMD", "curl", "-f", "http://localhost:4000/health/liveliness || exit 1" ]  # Command to execute for health check
      interval: 30s  # Perform health check every 30 seconds
      timeout: 10s   # Health check command times out after 10 seconds
      retries: 3     # Retry up to 3 times if health check fails
      start_period: 40s  # Wait 40 seconds after container start before beginning health checks
  # Reference: https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/.env.example
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    ports:
      - 7004:3001
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - SYS_ADMIN
    networks:
      - ai-network
    environment:
    # TODO: Potentially change to Embeddings
    # Adjust for your environment
      - SERVER_PORT=3001
      - STORAGE_DIR=/app/server/storage
      - JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
      # - LLM_PROVIDER=ollama
      # - OLLAMA_BASE_PATH=http://ollama:11434
      # - OLLAMA_MODEL_PREF=llama2
      # - OLLAMA_MODEL_TOKEN_LIMIT=8192
      - LLM_PROVIDER=litellm
      - LITE_LLM_MODEL_PREF=deepseek-chat
      - LITE_LLM_MODEL_TOKEN_LIMIT=8192
      - LITE_LLM_BASE_PATH=http://host.docker.internal:7001
      - LITE_LLM_API_KEY=${LITELLM_API_TOKEN}
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_BASE_PATH=http://host.docker.internal:11434
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest
      - EMBEDDING_MODEL_MAX_CHUNK_LENGTH=8192
      - VECTOR_DB=chroma
      - CHROMA_ENDPOINT=http://host.docker.internal:5001
      - CHROMA_API_HEADER=Authorization
      - CHROMA_API_KEY=${CHROMEDB_API_TOKEN}
      - WHISPER_PROVIDER=local
      - TTS_PROVIDER=native
      - PASSWORDMINCHAR=6
      - AGENT_SEARXNG_API_URL=http://searxng:7777
      # Add any other keys here for services or settings
      # you can find in the docker/.env.example file
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/anythingllm:/app/server/storage
    restart: always
    extra_hosts:
      - "host.docker.internal:host-gateway"
  n8n:
    container_name: n8n
    image: n8nio/n8n:latest
    restart: unless-stopped
    environment:
    - DB_TYPE=postgresdb
    - DB_POSTGRESDB_HOST=vector-admin-postgres
    - DB_POSTGRESDB_USER=vectoradmin
    - DB_POSTGRESDB_PASSWORD=${VECTOR_ADMIN_DB_PASSWORD}
    - DB_POSTGRESDB_DATABASE=n8n
    - N8N_DIAGNOSTICS_ENABLED=false
    - N8N_PERSONALIZATION_ENABLED=true
    - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
    - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
    ports:
      - 7002:5678
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/n8n/storage:/home/node/.n8n
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/n8n/backup:/backup
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/n8n/shared:/data/shared
    depends_on:
      - vector-admin-postgres
    networks:
      - ai-network
  flowise:
    image: flowiseai/flowise
    restart: unless-stopped
    container_name: flowise
    environment:
        - PORT=3001
    ports:
        - 7003:3001   
    volumes:
        - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/flowise:/root/.flowise
    entrypoint: /bin/sh -c "sleep 3; flowise start"
    networks:
      - ai-network
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    restart: unless-stopped
    ports:
      - 6333:6333
      - 6334:6334
    environment:
      - QDRANT__STORAGE__BACKUP__ENABLED=${QDRANT_API_TOKEN}
      - QDRANT__STORAGE__BACKUP__ENABLED=true
      - QDRANT__STORAGE__BACKUP__PATH=/qdrant/storage/backup
      - QDRANT__STORAGE__BACKUP__MAX_BACKUPS=5
      - QDRANT__STORAGE__BACKUP__MAX_BACKUP_SIZE=1000000000
      - QDRANT__STORAGE__BACKUP__MAX_BACKUP_AGE=604800
      - QDRANT__STORAGE__BACKUP__MAX_BACKUP_COUNT=5
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/qdrant/storage:/qdrant/storage
    networks:
      - ai-network
  chromadb-ui:
    image: fengzhichao/chromadb-admin
    platform: linux/arm64
    container_name: chromadb-ui
    restart: unless-stopped
    ports:
      - 5008:3000
    networks:
      - ai-network
  chromadb:
    image: chromadb/chroma:latest
    container_name: chromadb
    restart: unless-stopped # possible values are: "no", always", "on-failure", "unless-stopped"
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/chromadb:/chroma/chroma
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      # - transformer_api: "http://transformer:5000/v1/embed"
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma/chroma # this is the default path, change it as needed
      - ANONYMIZED_TELEMETRY=FALSE
      # - CHROMA_SERVER_AUTHN_CREDENTIALS=${CHROMEDB_API_TOKEN}
      # - CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=Authorization
      # - CHROMA_SERVER_AUTHN_PROVIDER=chromadb.auth.token_authn.TokenAuthenticationServerProvider
      - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMEDB_API_TOKEN}
      # - CHROMA_SERVER_AUTH_CREDENTIALS_FILE="./example_file"
      - CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER="chromadb.auth.token.TokenConfigServerAuthCredentialsProvider"
      - CHROMA_SERVER_AUTH_PROVIDER="chromadb.auth.token.TokenAuthServerProvider"
      - CHROMA_CLIENT_AUTH_PROVIDER="chromadb.auth.token_authn.TokenAuthClientProvider"
      - CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER="Authorization"
    ports:
      - 5001:8000
      # http://localhost:5001/docs
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/v2/heartbeat" ]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai-network
  vector-admin: # Template: https://github.com/3x3cut0r/vps/blob/main/docker/compose/vector-admin/docker-compose.yml
    image: mintplexlabs/vectoradmin
    container_name: vector-admin
    ports:
      - '7005:3001'
      - '3355:3355'
      - '8288:8288'
    env_file:
      - .env
    environment:
      - SERVER_PORT=3001
      - JWT_SECRET=${VECTOR_ADMIN_JWT_SECRET}
      - INNGEST_EVENT_KEY=background_workers
      - INNGEST_SIGNING_KEY=${VECTOR_ADMIN_INNGEST_SIGNING_KEY}
      - INNGEST_LANDING_PAGE=true
      - DATABASE_CONNECTION_STRING=${VECTOR_ADMIN_DB_CONNECTION_STRING}
    depends_on:
      - chromadb
      - qdrant
      - vector-admin-postgres
    volumes:
      - "${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/vector-admin/.env:/app/backend/.env"
      - "${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/vector-admin/backend/storage:/app/backend/storage"
      - "${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/vector-admin/document-processor/hotdir/:/app/document-processor/hotdir"
    networks:
      - ai-network
  vector-admin-postgres:
    container_name: vector-admin-postgres
    image: postgres:16-alpine
    restart: always
    expose:
      - "5433"
    ports:
      - "5433:5433"
    command: -p 5433
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/vector-admin/postgres:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: 'vectoradmin'
      POSTGRES_PASSWORD: '${VECTOR_ADMIN_DB_PASSWORD}'
      POSTGRES_DB: 'vectoradmin'
    networks:
      - ai-network
  nocodb:
    image: nocodb/nocodb:latest
    container_name: nocodb
    restart: unless-stopped
    ports:
      - "7006:8080"
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/nocodb:/usr/app/data
    environment:
      NC_DB: "pg://host.docker.internal:5433?u=vectoradmin&p=Ihaveastream68&d=nocodb"
      NC_DISABLE_TELE: "true"  # Optional: disables telemetry
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - ai-network
  redis: # Reference: https://docs.openwebui.com/tutorials/integrations/redis
    container_name: redis
    image: docker.io/valkey/valkey:8.0.1-alpine
    command: "valkey-server --save 30 1"
    healthcheck:
      test: "[ $$(valkey-cli ping) = 'PONG' ]"
      start_period: 5s
      interval: 1s
      timeout: 3s
      retries: 5
    restart: unless-stopped
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/valkey:/data
    networks:
      - ai-network
  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    ports:
      - 7777:8080
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=http://${SEARXNG_HOSTNAME:-localhost}/
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: json-file
      options:
        max-size: 4m
        max-file: "1"
    networks:
      - ai-network
  tika:
    container_name: tika
    image: apache/tika:latest-full
    restart: always
    ports:
      - 9998:9998
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - ai-network
    # volumes:
      # - ${DOCKER_VOLUME_STORAGE:-/mnt/nas/DockerServices}/_homelab/services/ai-server/tika-config.xml:/tika-config.xml
networks:
  ai-network:
    external: true
  